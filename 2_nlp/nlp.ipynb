{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Natural Language Processing in Risk and Finance </h1>\n",
    "<h3> <center> Developing a </h3>\n",
    "<center> <small>by <a href=\"https://juliandoerr.com\">Julian Dörr</a></small>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_________________________\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a id='toc1_'></a>[About this Course](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc0_'></a>    \n",
    "- 1. [About this Course](#toc1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=4\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find all course material and setup instructions in the following [repository](https://github.com/julienOlivier3/risk-analytics/tree/main/2_nlp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>What you will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Fundamental Concepts**: Overview of ML and its relationship with Artificial Intelligence (AI) and Deep Learning (DL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a specialized branch of AI where methods from the field of Machine Learning and Deep Learning are applied to bridge the gap between human communication and machine understanding. \n",
    "\n",
    "<img src=\"img/ai_ml_dl_nlp.png\" alt=\"AI, ML, DL & NLP\" style=\"width: 55vw; min-width: 330px;\">\n",
    "<p><small>Image source: Author</small></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#58e4d4\"><b>Artificial Intelligence (AI)</b></font>: **The Broad Umbrella**\n",
    "\n",
    "AI is the overarching field that encompasses all technologies and systems designed to **simulate human intelligence**. This includes tasks like reasoning, problem-solving, learning, and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#20b49c\"><b>Machine Learning (ML)</b></font>: **The Foundation**\n",
    "\n",
    "Machine learning is a subset of AI that enables systems to **learn from data** and improve their performance over time without being explicitly programmed. ML algorithms identify patterns in data and use these patterns to **make predictions or decisions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<font color=\"#086c5c\"><b>Deep Learning (DL)</b></font>: **The Engine Behind Generative AI**\n",
    "\n",
    "Deep learning is a specialized branch of ML that uses **artificial neural networks** inspired by the human brain. These networks are particularly effective at processing large amounts of **unstructured data**, such as images, text, and audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<font color=\"#08544c\"><b>Natural Language Processing (NLP)</b></font>: **A Key Application Area**\n",
    "\n",
    "Natural language processing is a **specialized branch of AI** that focuses on **enabling machines to understand, interpret, and generate human language**. It bridges the gap between human communication and machine understanding.\n",
    "\n",
    "NLP draws on concepts from linguistics, computer science, and AI to process and analyze natural language data. It is often **powered by ML and DL techniques**, which help machines learn from vast amounts of text data and improve their language understanding capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>Application of Natural Language Processing in Risk Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP in risk management enhances the ability to **analyze** unstructured data, such as **news articles, social media, and customer feedback, to identify potential risks, emerging trends, and fraudulent activities**. By leveraging techniques like sentiment analysis, named entity recognition, and text summarization, NLP can **automate the extraction of relevant information**, improve accuracy, and **enable faster response times**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>A real-world use case: Automated Claims Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will work on a real-world use case that **insurance companies** typically face. It is the process of **reviewing an insurance claim submitted by policyholders** to the insurance company. \n",
    "\n",
    "Instead of dealing with claims manually, **NLP algorithms are used to extract relevant information** from unstructured data sources: claim forms, emails, and documents. Once done, they **automatically categorise and prioritise claims** based on their severity and complexity, ensuring that urgent or complex claims receive prompt attention while routine claims are processed efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "> An **insurance claim** is a formal request made by a policyholder to an insurance company for coverage for losses or damages incurred that are covered under the insurance policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Claims processing is a multi-step process which bears great potential for automation using NLP techniques.\n",
    "\n",
    "<img src=\"img/claims_processing.png\" alt=\"Claims processing\" width=\"1000\" height=\"300\"/>\n",
    "<p><small>Image source: <a href=\"https://www.astera.com/de/type/blog/automated-claims-processing/\">Astera</a></small></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>Developing an Automated Claims Processing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims = pd.read_csv('data/claims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE IV WAS MAKING A LEFT TURN ON A GREEN ARROW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLAIMANT ALLEGES SHE SUFFERED INJURIES IN AN E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IV PASSENGER SUSTAINED INJURIES, OV AND IV COL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   claim_description\n",
       "0  THE IV WAS MAKING A LEFT TURN ON A GREEN ARROW...\n",
       "1  CLAIMANT ALLEGES SHE SUFFERED INJURIES IN AN E...\n",
       "2  IV PASSENGER SUSTAINED INJURIES, OV AND IV COL..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_claims.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(227)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_claims.claim_description.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims = df_claims[df_claims.claim_description.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191463"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_claims.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data comprising 191,463 claim descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercase all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims['claim_description'] = df_claims.claim_description.apply(lambda text: text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dc066\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dc066_level0_col0\" class=\"col_heading level0 col0\" >claim_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dc066_level0_row0\" class=\"row_heading level0 row0\" >153438</th>\n",
       "      <td id=\"T_dc066_row0_col0\" class=\"data row0 col0\" >due to collision causing damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dc066_level0_row1\" class=\"row_heading level0 row1\" >131796</th>\n",
       "      <td id=\"T_dc066_row1_col0\" class=\"data row1 col0\" >orly backed out of a parking spot and backed into the ov. ov is   being worked on at the shop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dc066_level0_row2\" class=\"row_heading level0 row2\" >153802</th>\n",
       "      <td id=\"T_dc066_row2_col0\" class=\"data row2 col0\" >rock from road - no one at fault d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x108eda3ac60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_claims.sample(3).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_claims.claim_description.apply(lambda x: len(x.split(' '))).plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization in NLP is the process of **breaking down text into smaller units called tokens**, which can be **words, phrases, or symbols**, and it is essential for enabling machines to analyze and understand unstructured text data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we use the `re` module to define our custom tokenization logic. Regex is a powerful tool for string manipulation and can be used to extract tokens from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Regular expressions** (regex) are sequences of characters that define search patterns, allowing users to efficiently find, match, or manipulate strings of text based on specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokenize_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\w: **word character** like letters (both lowercase and uppercase), digits or underscores. \\w\\w+ means that at least 2 word characters need to follow one another.\n",
    "- \\b: **word boundary** position where a word character is not followed or preceded by another word character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenize_pattern.findall('This is a test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'test']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords are common words in a language**, such as \"the,\" \"is,\" and \"and,\" that **carry little semantic value** and are often removed in NLP tasks to enhance the efficiency and accuracy of text analysis by focusing on more meaningful content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are pre-defined lists of stopwords for different languages. We use the English stopword list from the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en = [stopword.lower() for stopword in stopwords.words('english')]\n",
    "\n",
    "len(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tokenize the claims descriptions and remove the stopwords. Moreover, we count how often each word occurs in the corpus of claims descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In NLP, a **corpus** is a large and structured collection of authentic text data used for training, testing, and evaluating NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191463/191463 [00:16<00:00, 11470.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "# Process each claim and update the word counter\n",
    "for claim in tqdm(df_claims.claim_description.values):\n",
    "    # Split the claim into words using the regex pattern\n",
    "    words = tokenize_pattern.findall(claim)\n",
    "\n",
    "    # Filter out empty strings and stopwords and update the counter\n",
    "    word_counter.update(word for word in words if word and word not in stopwords_en)\n",
    "\n",
    "# Convert the Counter to a dictionary\n",
    "word_frequencies = dict(word_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common words found in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iv', 76356),\n",
       " ('vehicle', 55823),\n",
       " ('damage', 47182),\n",
       " ('ov', 46567),\n",
       " ('driver', 46394),\n",
       " ('injuries', 38978),\n",
       " ('claimant', 31274),\n",
       " ('rear', 28882),\n",
       " ('front', 27656),\n",
       " ('struck', 26369),\n",
       " ('reported', 26286),\n",
       " ('customer', 23884),\n",
       " ('hit', 23685),\n",
       " ('left', 21118),\n",
       " ('causing', 20370),\n",
       " ('side', 20037),\n",
       " ('right', 16849),\n",
       " ('fell', 16566),\n",
       " ('parked', 15541),\n",
       " ('incurred', 14004)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a clear picture of the domain of the text data: Insurance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the vocabulary which is essential for defining the scope of language the claims processing model can understand and process effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A **vocabulary** is a set of unique words in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(set(word_frequencies.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary, comprises 93,231 distinct words which is deemed to be a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Oxford Dictionary includes about 273,000 headwords, with 171,476 currently in use, 47,156 obsolete, and around 9,500 derivatives. It features over 600,000 total word forms, while some estimates suggest the English vocabulary may reach 1 million words, including specialized and foreign terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look which words have entered our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '00000',\n",
       " '0000000115192chef',\n",
       " '0000000115196',\n",
       " '000001',\n",
       " '000001593',\n",
       " '000007',\n",
       " '000019',\n",
       " '000023',\n",
       " '000025',\n",
       " '000026',\n",
       " '000028',\n",
       " '000029',\n",
       " '000031',\n",
       " '000034',\n",
       " '000051',\n",
       " '00005753',\n",
       " '000058']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our tokenizer definition number sequences are also extracted as tokens (remember that \\w matches **word character** like letters *and* digits). Clearly, we do not want these number sequences as part of our vocabulary. Thus, we remove tokens from our vocabulary which are not part of the official English dictionary. Again, we make use of `nltk` which provides an extensive list of English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "nltk.download('words', quiet=True)\n",
    "dictionary = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {word.lower() for word in dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [word for word in vocabulary if word in dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16283"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with a vocabulary of 16,283 distinct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zip',\n",
       " 'zipper',\n",
       " 'zonar',\n",
       " 'zone',\n",
       " 'zoned',\n",
       " 'zoning',\n",
       " 'zoo',\n",
       " 'zoom',\n",
       " 'zoster',\n",
       " 'zucchini']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest methods for text vectorization is the **bag-of-words (BoW) representation**, where a BoW **vector has a length equal to the entire vocabulary**, $V$, and its **values** indicate the **frequency of each word**'s occurrence, $tf$, in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW vectorization\n",
    "\n",
    "<img src=\"img/tf.png\" alt=\"BoW\" width=\"600\">\n",
    "<p><small>Image source: Author</small></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we vectorize the corpus of claims descriptions via the number of occurences of each word from the vocabulary by using `scikit-learn`'s `CountVectorizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=vocabulary, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df_claims.claim_description.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fitted `vectorizer`, we can now transform any string into a count vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = 'Broken rear window while parked. Window splinter caused damage to other vehicle.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.transform([claim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "x<sub>(1x16283)</sub> = [0 0 0 ... 1 ... 1 ... 1 ... 1 ... 1 ... 2 ... 0 0 0]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken   : 1884  \t-> 1\n",
      "damage   : 3708  \t-> 1\n",
      "rear     : 11408 \t-> 1\n",
      "splinter : 13421 \t-> 1\n",
      "vehicle  : 15535 \t-> 1\n",
      "window   : 16035 \t-> 2\n"
     ]
    }
   ],
   "source": [
    "from util import print_sparse_vector\n",
    "\n",
    "print_sparse_vector(x, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector has $V$ = 16,283 elements with only 6 of it being non-zero. High-dimensional vectors with predominantly zero values are called **sparse vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words of the above claim are not part of the vocabulary because they are not in the dictionary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'parked' and 'caused' in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or because they are stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'while' and 'to' and 'other' in stopwords_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted word count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Bag-of-Words** techniques like TF-IDF (Term Frequency-Inverse Document Frequency) **assign higher relevance to words that appear in fewer documents**, emphasizing their uniqueness by comparing a word's frequency in a specific text to its overall frequency in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf vectorization\n",
    "\n",
    "<img src=\"img/tf-idf.png\" alt=\"Tf-idf\" width=\"800\">\n",
    "<p><small>Image source: Author</small></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **scikit-learn**'s **TfidfVectorizer** to get a weighted term frequency representation of the claims descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary, lowercase=True, use_idf=True, smooth_idf=False, sublinear_tf=False, norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df_claims.claim_description.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.transform([claim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Vector:<br> x<sub>(1x16283)</sub> = [0.0 0.0 0.0 ... 4.8 ... 2.6 ... 3.1 ... 9.8 ... 2.7 ... 11.5 ... 0.0 0.0 0.0]'"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero elements:\n",
      "broken   : 1884  \t-> 4.818420285039794\n",
      "damage   : 3708  \t-> 2.5850998857626366\n",
      "rear     : 11408 \t-> 3.134578877620683\n",
      "splinter : 13421 \t-> 9.761252475784687\n",
      "vehicle  : 15535 \t-> 2.6984041040430347\n",
      "window   : 16035 \t-> 11.53960467345044\n"
     ]
    }
   ],
   "source": [
    "from util import print_sparse_vector\n",
    "\n",
    "print_sparse_vector(x, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf value is derived as follows:\n",
    "\n",
    "$tf\\text{-}idf = term \\, frequency \\times log\\left(\\frac{number \\, of \\, documents}{document \\, frequency}\\right) + 1 = tf \\times log\\left(\\frac{N}{df}\\right) + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word 'broken' the value can be derived as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4205)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = re.compile(r'(?u)\\bbroken\\b')\n",
    "\n",
    "df = df_claims.claim_description.apply(lambda text: bool(word.search(text))).sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191463"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = df_claims.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.818420285039794)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf * np.log(N/df) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse, count-based methods mentioned earlier overlook the meanings of words and phrases. Words are not just letter combinations; they carry meanings and usage contexts that reflect their semantics, which go beyond their basic lexical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following vectorization techniques capture exactly those semantic properties of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You shall know a word by the company it keeps!\" <br> \n",
    "*Firth (1957)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that in order to represent the semantic meaning of a word, knowing its surrounding words is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec uses neural language modelling to generate vector representations of words. These vectors encapsulate the meaning of a word by considering the context provided by adjacent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec comes in many variants. The variant below employs a shallow neural network where the learning task is to predict surrounding words given a target word as input. This approach is called Continuous Bag of Words (CBoW).\n",
    "\n",
    "<img src=\"img/word2vec.png\" alt=\"Word2vec\" width=\"800\">\n",
    "<p><small>Image source: Author</small></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vector representations are a by-product of the training process generated in the weights matrix in the hidden layer. The word vector representations are called word embeddings.\n",
    "\n",
    "<img src=\"img/word2vec_embedding.png\" alt=\"Word embedding\" width=\"800\">\n",
    "<p><small>Image source: Author</small></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "word2vec = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_load_specials',\n",
       " '_log_evaluate_word_analogies',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " '_upconvert_old_d2vkv',\n",
       " '_upconvert_old_vocab',\n",
       " 'add_lifecycle_event',\n",
       " 'add_vector',\n",
       " 'add_vectors',\n",
       " 'allocate_vecattrs',\n",
       " 'closer_than',\n",
       " 'cosine_similarities',\n",
       " 'distance',\n",
       " 'distances',\n",
       " 'doesnt_match',\n",
       " 'evaluate_word_analogies',\n",
       " 'evaluate_word_pairs',\n",
       " 'expandos',\n",
       " 'fill_norms',\n",
       " 'get_index',\n",
       " 'get_mean_vector',\n",
       " 'get_normed_vectors',\n",
       " 'get_vecattr',\n",
       " 'get_vector',\n",
       " 'has_index_for',\n",
       " 'index2entity',\n",
       " 'index2word',\n",
       " 'index_to_key',\n",
       " 'init_sims',\n",
       " 'intersect_word2vec_format',\n",
       " 'key_to_index',\n",
       " 'lifecycle_events',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'log_evaluate_word_pairs',\n",
       " 'mapfile_path',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'most_similar_to_given',\n",
       " 'n_similarity',\n",
       " 'next_index',\n",
       " 'norms',\n",
       " 'rank',\n",
       " 'rank_by_centrality',\n",
       " 'relative_cosine_similarity',\n",
       " 'resize_vectors',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'set_vecattr',\n",
       " 'similar_by_key',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'similarity_unseen_docs',\n",
       " 'sort_by_descending_frequency',\n",
       " 'unit_normalize_all',\n",
       " 'vector_size',\n",
       " 'vectors',\n",
       " 'vectors_for_all',\n",
       " 'vectors_norm',\n",
       " 'vocab',\n",
       " 'wmdistance',\n",
       " 'word_vec',\n",
       " 'words_closer_than']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = word2vec.get_vector(\"splinter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523604273796082),\n",
       " ('throne', 0.7664334177970886),\n",
       " ('prince', 0.7592144012451172),\n",
       " ('daughter', 0.7473883628845215),\n",
       " ('elizabeth', 0.7460219860076904),\n",
       " ('princess', 0.7424570322036743),\n",
       " ('kingdom', 0.7337412238121033),\n",
       " ('monarch', 0.721449077129364),\n",
       " ('eldest', 0.7184861898422241),\n",
       " ('widow', 0.7099431157112122)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Vector:<br> x<sub>(1x50)</sub> = [1.1339 -0.7375 0.5622 ... -0.3817 -0.5179 -0.9973]'"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util import print_vector\n",
    "\n",
    "print_vector(x, rounding_digit=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
